% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{calligra}
\usepackage{calrsfs}
\usepackage{subcaption}
\usepackage{listings}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{HW7}
\author{Carl Mueller\\ %replace with your name
CSCI 5254 - Convex Optimization} %if necessary, replace with your course title
\maketitle

\subsection*{8.16}
Formulate the following asa CVX optimization problem:\\
Find the rectangle
$$R = \set{x \in \textbf{R}^n | l \preceq x \preceq u}$$
of maximum volume enclosed in the polyhedron
$$P = \set{x | Ax \preceq b}$$\\
The volume can be expressed as:
\begin{proposition}{1}
\begin{align}
v = \prod_{i=1}^{n}u_i-l_i
\end{align}
\end{proposition}
We want all the $2^n$ corners do be contained within the polyhedron. This every corner must meet the polyhedron constraint $Ac \preceq b$. Where $c$ is the vector of corners. Each of these corners can be be more succinctly represented as the vector based on the upper and lower values of each edge:\\ 
If we express $x_i$ as $u_i-l_i$ then this system becomes
$$\sum_{i=1}^{n}a_{ij}(u_j-l_j) \le b_i$$\\
The problem can be expressed as:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & \prod_{i=1}^{n}u_i-l_i\\
& \text{subject to}\
& &\sum_{i=1}^{n}a_{ij}(u_j-l_j) \le b_i
\end{aligned}
\end{equation*}
The constraint is a posynomial as it is a summation of the monomial $a_{ij}(u_j-l_j)$.\\
To make the problem a non-linear geometric optimization problem, we take the log of the objective:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & \sum_{i=1}^{n}log(u_i-l_i)\\
& \text{subject to}\
& &\sum_{i=1}^{n}a_{ij}(u_j-l_j) \le b_i
\end{aligned}
\end{equation*}

\subsection*{8.24}
We make use of the Cauchy-Schwarz inequality and sustite $p$ knowing that $||u||_2 \le p$:
\begin{proposition}{1}
\begin{align}
u^tx_i \le ||u||_2|x_i||_2\\
||u||_2||x_i||_2 \le p||x_i||_2\\
u^ty_j \le ||u||_2|y_j||_2\\
||u||_2|y_j||_2 \le p||y_j||_2\\
-||u||_2||y_j||_2 \ge -p||y_j||_2\\
\end{align}
\end{proposition}
For $x_i$:
\begin{equation*}
\begin{aligned}
(a+u)^Tx_i \ge b\\
a^Tx_i+u^Tx_i \ge b\\
a^Tx_i+||u||_2|x_i||_2 \ge b\\
a^Tx_i+p|x_i||_2 \ge b\\
a^Tx_i-b \ge -p|x_i||_2
\end{aligned}
\end{equation*}
For $x_i$:
\begin{equation*}
\begin{aligned}
(a+u)^Ty_j \le b\\
a^Ty_j+u^Ty_j \le b\\
a^Ty_j+||u||_2||y_j||_2 \le b\\
a^Ty_j+p||x_i||_2 \le b\\
a^Ty_j-b \le -p||y_j||_2\\
b-a^Ty_j \ge p||y_j||_2
\end{aligned}
\end{equation*}
The optimization problem:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & p\\
& \text{subject to}\
& & b-a^Ty_j \ge p||y_j||_2\\
&&& a^Tx_i-b \ge -p|x_i||_2\\
&&& ||a||_2 \le 1
\end{aligned}
\end{equation*}\\
\textbf{Additional Exercises:}\\
\subsection*{5.12}
One heurisitc estiamte an initial $\hat{x}$ using the huber penalty function. We then use that $\hat{x}$ to estimate a $\hat{P}$ by aligning the indices of $Ax$ and $y$ to find a permutation matrix. Then using that same permutation we reoptimized for $\hat{x}$. We repeat this algorithm until the euclidean norm of the distance between the $\hat{x}_{\tau}$ and $\hat{x}_{\tau-1}$ is below some tolerance, $\tau$ being the current iteration step.
\textbf{Code:}\\
\begin{lstlisting}
above_tol = 1
tolerance = .00000001
% Seed our initial estimate of x using huber function
cvx_begin
variable x(n);
    minimize( sum(huber(A*x-y)) );
cvx_end
P_hat = eye(m)
x_prior = zeros(n)
while 1
    
    % Align the smallest indixes, find pi (the permutation index alignement) 
    % and construct the permutation matrix P_hat accordingly:
    [Ax_values, Ax_idx] = sort(A*x);
    [y_values, y_idx] = sort(y);
    pi = [y_idx' ; Ax_idx'];
    P_temp = zeros(m, m);
    for i = 1 : m
       row = pi(1,i);
       col = pi(2,i);
       P_temp(row, col) = 1;
    end
    P_hat = P_temp;
    if P_hat*P_hat' ~= eye(m)
        "Invalid P_hat!"
        break
    end
    "Distance:"
    dist = norm(x - x_prior, 2)
    if dist <= tolerance
        break
    end
    x_prior = x;
    
    % Find x_hat
    cvx_begin
        variable x(n,1)
        minimize(norm(A*x-P_hat'*y, 2))
    cvx_end;
end
P_eye = eye(m);
cvx_begin
    variable x_eye(n,1)
    minimize(norm(A*x_eye-P_eye'*y, 2))
cvx_end;
"Distance x (P=I) and estimated x:"
norm(x_eye - x_true, 2)
"Distance x_true and estimated x:"
norm(x_true - x, 2)
\end{lstlisting}
\textbf{Results:}\\
"Distance estimated x (P=I) and x\_true:"\\
ans = 3.4363\\
"Distance x\_true and estimated x:"\\
ans = 0.0965\\
\subsection*{5.18}
We can reformulate the problem as the original object being less than or equal to some value $z$:
\begin{equation*}
\begin{aligned}
1+max_{k\neq y_i} f_k(x_i)  - f_{y_i} (x_i) \leq z_i,  z_i\geq 0\\
\end{aligned}
\end{equation*}\\
This can be represented by the following problem:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& &  \sum_i z_i + \mu ||A||_F^2\\
& \text{subject to}\
& & 1+max_{k\neq y_i} f_k(x_i) - f_{y_i} (x_i) \leq z_i, \forall i\\
&&&  1^T b= 0,   z \geq 0\\
\end{aligned}
\end{equation*}\\
This can be reexpressed using the individual inequality constraints:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& &  \sum_i z_i + \mu ||A||_F^2\\
& \text{subject to}\
& &  1^T b= 0,   z \geq 0\\
&&&  1+a_k^Tx_i + b  - y_i \leq z_i,    k = 1, 2, …,  y_i -1, y_i +1, …, K, i=1, 2, …, m
\end{aligned}
\end{equation*}\\
\begin{figure}[h]
\includegraphics[scale=.25]{Problem_5_18.jpg}
\end{figure}
\textbf{Code:}\\
\begin{lstlisting}
E = [];
U = [0.01 0.05 0.1 0.2 0.5 1 2 5 10 20 50 75 100]
% This loop generates a new u value. 
for u = 1:size(U,2)
    cvx_begin
        variable z(mTrain, 1)
        variable A(K, n)
        variable b(K, 1)
        minimize(sum(z) + U(u)*square_pos(norm(A,'fro')))
        subject to
        for i=1:mTrain
            for k=[1:y(i)-1 y(i)+1:K]
                1+(A(k,:)*x(:,i)+b(k))-(A(y(i),:)*x(:,i)+b(y(i))) <= z(i);
            end
            z(i) >= 0;
        end
        sum(b) == 0;
    cvx_end
    % Compute the predict predicted labels by computing the affine function
    % on xtest using the estimated optial A and b. Find the max in each
    % column (i.e. argmax for label) and round to get whole number value.
    correct = 0
    y_pred = zeros(1,mTest);

    for i=1:mTest
        [~, y_pred(i)] = max(A*xtest(:,i) + b);
        if (y_pred(i) == ytest(i))
            correct = correct + 1;
        end
    end
    percent_correct = correct/mTest
    E = [E ; percent_correct]    
end
plot(U,E)
\end{lstlisting}


\subsection*{13.15}
One heurisitic is to ensure that the 1-norm of $w$ is minimized. We can then formulate and optimization problem subject to the following constraint:
\begin{proposition}{1}
\begin{align}
E[(r-\bar{r})((r-\bar{r})] = \Sigma\\
E[rr^T] - \bar{r}\bar{r}^T = \Sigma\\
E[rr^T] = \Sigma + \bar{r}\bar{r}^T
\end{align}
\end{proposition}
\begin{proposition}{2}
\begin{align}
E[z^Tz] = c^Tdiag(\Sigma) + \bar{r}^Tcc^T\bar{r}
\end{align}
\end{proposition}
\begin{equation*}
\begin{aligned}
E[(z-w^Tr)(z-w^Tr)] \le .01E[z^2]\\
E[z^Tz + r^Tww^Tr - 2zw^Tr ] \le .01E[z^2]\\
E[z^Tz] + E[r^Tww^Tr] - 2E[zw^Tr] \le .01E[z^2]\\
E[z^Tz] + E[r^Tww^Tr] - 2E[zw^Tr] \le .01E[z^2]\\
E[z^Tz] + E[w^Trr^Tw] - 2E[(c^Tr)^Tw^Tr] \le .01E[z^2]\\
c^Tdiag(\Sigma) + \bar{r}^Tcc^T\bar{r} + w^T(\Sigma + \bar{r}\bar{r}^T)w - 2E[(c^Tr)^Tw^Tr] \le .01E[z^2]\\
c^Tdiag(\Sigma) + \bar{r}^Tcc^T\bar{r} + w^T(\Sigma + \bar{r}\bar{r}^T)w - 2w^T(\Sigma + \bar{r}\bar{r}^T)c \le .01E[z^2]\\
\end{aligned}
\end{equation*}\\
This becomes the optimization problem reflected in the matlab code:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& &  ||w||_1\\
& \text{subject to}\
& & c^Tdiag(\Sigma) + \bar{r}^Tcc^T\bar{r} + w^T(\Sigma + \bar{r}\bar{r}^T)w - 2w^T(\Sigma + \bar{r}\bar{r}^T)c \le .01E[z^2]\\
\end{aligned}
\end{equation*}\\
\textbf{Code:}\\
\begin{lstlisting}
ctc = mtimes(c', c);
rbarsq = dot(rbar, rbar');
zsqr = (ctc * rbarsq)
cvx_begin
  variable w(n)
  minimize norm(w,1)
  E_num = ( (rbar' * (c * c') * rbar)) + c'*diag(Sigma) + (w' * (Sigma + (rbar * rbar')) * w) - 2 * w' *(Sigma + (rbar * rbar')) * c ;
  subject to
      E_num <= 0.01 * (ctc * rbarsq);
      w <= c;
cvx_end
E_num/(ctc * rbarsq)
sum(abs(w > 0.01))
sum(abs(c > 0.01))
\end{lstlisting}
\textbf{Results:}\\
ans = 0.0100\\
ans = 108\\
ans = 500\\

\subsection*{16.5}
We begin by stating that $X = \theta s$. The optmization problem can be formulated as: 
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & \sum_{t=1}^{T}\phi(s_t) \\
& \text{subject to}\
& & S_{min} \preceq s \preceq S_{max}\\
&&& |s_{t+1}-s_t| \le R\\
&&& s = X1, X^T1 \succeq W, X \succeq 0\\
&&& X_{ti} = 0, t = 1,\dots, A_i-1, 1=i,\dots,n\\
&&& X_{ti} = 0, t = D_i+1,\dots, T, 1=i,\dots,n\\
\end{aligned}
\end{equation*}\\
The top two constraitns are the processor speed limits and slew rates. Each row coefficients $\theta_{ti}$ for $1=1,\dots,n$ must sum to one therefore $X1=s$. The last two constraints express that each component of  $X_{ti}$ is 0 for any values outside the range $[A_i,D_i]$ for the given job $i.$
\textbf{Code:}\\
\begin{lstlisting}
cvx_begin
    variable X(T,n)
    s = sum(X');
    minimize(sum(alpha+beta*s+gamma*square(s)))
    subject to
        X >= 0;
        Smin <= s <= Smax;
        abs(s(2:end)-s(1:end-1))<=R; % slew rate constraint
        % Timing constraints for each job
        for i=1:n
            for t=1:A(i)-1
                X(t,i)==0;
            end
            for t=D(i)+1:T
                X(t,i)==0;
            end
        end
        sum(X)>=W';
cvx_end
theta = X./(s'*ones(1,n));
figure;
bar((s'*ones(1,n)).*theta,1,'stacked');
xlabel('Time: tt');
ylabel('Stacked speed: s_t');
\end{lstlisting}
\textbf{Results:}\\
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{Problem_16_5_1.jpg}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{Problem_16_5_2.jpg}
\caption{Stacked processor speed, each stack representing portion of speed devoted to allocated jobs.}
\end{figure}

\subsection*{17.4}
We must maximize the net revenue minus the penalty to maximize the net profit:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{maximize}}
& & trace(R^TN) - p^Ts\\
& \text{subject to}\
& & s = q - A^TNT\\
&&& N \succeq 0\\
&&& \sum_{t=1}^{T}N_{it} = I_{i}
\end{aligned}
\end{equation*}\\
\textbf{Code:}\\
\begin{lstlisting}
cvx_begin
    variable N(n,T)
    % we need s positive to ensure no negative penalties.
    s =pos(q-diag(Acontr'*N*Tcontr))
    maximize(sum(diag(R'*N())) - p'*s)
    subject to
        N >= 0;
        sum(N)== I';
cvx_end

net_profit = cvx_optval
revenue = sum(diag(R'*N))
payment = p'*s

% Highest ad revenue %
cvx_begin
    variable N(n,T)
    maximize (sum(diag(R'*N)))
    subject to
        N >= 0;
        ones(1,n)*N == I';
cvx_end
hi_ad_revenue = cvx_optval;
s = pos(q-diag(Acontr'*N*Tcontr))
hi_ad_payment = p'*s
hi_ad_net_profit = hi_ad_revenue-hi_ad_payment
\end{lstlisting}
\textbf{Results:}\\
Full ad porfolio:\\
net\_profit = 230.5660\\
revenue = 268.2319\\
payment = 37.6659\\\\
Highest earning ad portfolio:\\
hi\_ad\_revenue = 305.1017 \\
hi\_ad\_payment = 232.2602 \\
hi\_ad\_net\_profit = 72.8415 \\

\subsection*{17.5}
For the first condition: $\phi(v) = v^2$ which will penalize higher values.
For the second condition: $\phi(v) = huber(v)$ which is less sensitive to outliers and thus will allow a few large preference violations.
The minimization problem for the squared error becomes becomes: 
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & \sum\phi(V)\\
& \text{subject to}\
& & V = max(r_j + 1 - r_i, 0)
\end{aligned}
\end{equation*}\\
For the huber penalty function, since CVX requires huber be passed an affine function, we must reformulate:
\begin{equation*}
\begin{aligned}
& \underset{}{\text{minimize}}
& & \sum\phi(V)\\
& \text{subject to}\
& & r_j + 1 - r_i, 0 \ge 0
\end{aligned}
\end{equation*}\\
\textbf{Code:}\\
\begin{lstlisting}
cvx_begin
variable R(50)
V = max(R(preferences(:,2))+1-R(preferences(:,1)),0)
minimize(sum(square_phi(V)))
cvx_end
sum(V>0.001)
histogram(V)

cvx_begin
variable R(50)
V = R(preferences(:,2))+1-R(preferences(:,1))
minimize(sum(huber_phi(V)))
subject to
    R(preferences(:,2))+1-R(preferences(:,1)) >= 0
cvx_end
sum(V>0.001)
histogram(V)

function square_penalty = square_phi( x )
square_penalty = pow_pos(x, 2)
end

function huber_penalty = huber_phi( x )
huber_penalty = huber(x)
end

\end{lstlisting}
\textbf{Results:}\\
Squared penalty:\\
sum(V>0.001) = 781\\
Huber Penalty:\\
sum(V>0.001) = 900

\begin{figure}[h]
\centering
\includegraphics[scale=.25]{histogram_square_penalty.jpg}
\caption{Histogram for square penalty.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=.25]{histogram_huber_penalty.jpg}
\caption{Histogram for huber penalty.}
\end{figure}

\subsection*{17.8}
\subsection*{17.9}
% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}