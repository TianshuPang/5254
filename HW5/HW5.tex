% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{calligra}
\usepackage{calrsfs}
\usepackage{subcaption}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{HW5}
\author{Carl Mueller\\ %replace with your name
CSCI 5254 - Convex Optimization} %if necessary, replace with your course title
\maketitle

\subsection*{6.1}
\begin{proposition}{1}
\begin{align}
log(x+1) \le x\\ -log(x+1) \ge x\\ x > -1
\end{align}
\end{proposition}

\begin{proposition}{2}
\begin{align}
-log(1-x)\ \text{is convex}
\end{align}
\end{proposition}
\begin{proposition}{3}
\begin{align}
\phi(||u||_{\infty}) = -a^2log(1-\frac{||u||_{\infty}}{a^2})
\end{align}
\end{proposition}

Left inequality working backwards:
\begin{equation*}
\begin{aligned}
||u||_{2}^{2} \le -a^2\sum_{i=1}^{m}log(1 - \frac{u_{i}^{2}}{a^2})\\
\sum_{i=1}^{m}\frac{|u_i|^2}{a^2} \le -\sum_{i=1}^{m}log(1 - \frac{u_{i}^{2}}{a^2})\\
\end{aligned}
\end{equation*}
Given proposition 1:
\begin{equation*}
\begin{aligned}
\sum_{i=1}^{m}\frac{|u_i|^2}{a^2} \le -\sum_{i=1}^{m}log(1 - \frac{u_{i}^{2}}{a^2})\\
\text{true when: }\\
- \frac{u_{i}^{2}}{a^2} \ge -1
\end{aligned}
\end{equation*}

Right inequality:
\begin{equation*}
\begin{aligned}
\text{Given: } u_{i}^{2} \le ||u_i||_{\infty}^{2}\\
\sum_{i=1}^{m}-log(1 - \frac{u_{i}^{2}}{a^2}) \le \sum_{i=1}^{m}-log(1 - \frac{||u_{i}||_{\infty}^{2}}{a^2})\ \text{given proposition 1}\\
\sum_{i=1}^{m}-log(1 - \frac{u_{i}^{2}}{a^2}) \le \frac{u_{i}^{2}}{||u||_{2}^{\infty}}\sum_{i=1}^{m}-log(1 - \frac{||u_{i}||_{\infty}^{2}}{a^2})\ \text{given $\frac{u_{i}^{2}}{||u||_{2}^{\infty}} \ge 1$}\\
-a^2\sum_{i=1}^{m}log(1 - \frac{u_{i}^{2}}{a^2}) \le -a^2\frac{u_{i}^{2}}{||u||_{2}^{\infty}}\sum_{i=1}^{m}log(1 - \frac{||u_{i}||_{\infty}^{2}}{a^2})\\
-a^2\sum_{i=1}^{m}log(1 - \frac{u_{i}^{2}}{a^2}) \le \frac{u_{i}^{2}}{||u||_{2}^{\infty}}\phi(||u||_{\infty})\\
\end{aligned}
\end{equation*}

\subsection*{6.9}
To show convexity, the following level set must be convex:

$$S_{\alpha} = \set{t_i| \max_{i=1,\dots,k}|\frac{p(t_i)}{q(t_i)} - y_i| \le \alpha}$$

Due to absolute value, following inequalities must hold:
\begin{equation*}
\begin{aligned}
-\alpha q(t_i) \le y_i q(t_i)-p(t_i) \le \alpha q(t_i)
\end{aligned}
\end{equation*}
 This is represent two inequalities that define a polyhedron and is therefore convex. Since the level set is convex,
 the original minimzation problem is at least quasiconvex.

\subsection*{7.3}

\begin{proposition}{1}
\begin{align}
P(x|y=1) = \frac{1}{\sqrt{2 \pi}}\int_{x}^{\infty}e^{\frac{-z^2}{2}}dz\\
P(x|y=0) = 1 - \frac{1}{\sqrt{2 \pi}}\int_{x}^{\infty}e^{\frac{-z^2}{2}}dz
\end{align}
\end{proposition}

Ordering probability terms in order of $y=1$ and $y=0$, our total probability is:
\begin{equation*}
\begin{aligned}
p(a,b) = \prod_{i=1}^{q}P_i(a^Tu_i+b|y=1)\prod_{i=q+1}^{m}(1-P_i(a^Tu_i+b|y=0))\\
\end{aligned}
\end{equation*}
The negative log likelihood:
\begin{equation*}
\begin{aligned}
l(a,b) = \sum_{i=1}^{q}-log(P_i(a^Tu_i+b|y=1))+\sum_{i=q+1}^{m}-log(1-P_i(a^Tu_i+b|y=0))
\end{aligned}
\end{equation*}
The negative log likelihood is a convex function, so minimizing this function is a convex optimization problem.

\subsection*{7.4}
\subsubsection*{a)}
\begin{proposition}{1}
\begin{align}
\text{Sample mean: }
u = \frac{1}{N}\sum_{k=1}^{N}y_k\\
\text{Covariance: }
Y = \frac{1}{N}\sum_{k=1}^{N}(y_k-u)(y_k-u)^T
\end{align}
\end{proposition}

\begin{equation*}
\begin{aligned}
-\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}R^{-1}\sum_{k=1}^{N}(y_k-a)(y_k-a)^T\\
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}R^{-1}\sum_{k=1}^{N}(y_ky_k^T-ay_k^T-y_ka^T+aa^T)\\
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}R^{-1}(\sum_{k=1}^{N}y_ky_k^T-\sum_{k=1}^{N}ay_k^T-\sum_{k=1}^{N}y_ka^T+\sum_{k=1}^{N}aa^T))\\
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}R^{-1}(\sum_{k=1}^{N}y_ky_k^T-\sum_{k=1}^{N}ay_k^T-\sum_{k=1}^{N}y_ka^T+Naa^T)\\
\end{aligned}
\end{equation*}

Substitute sample mean:
\begin{equation*}
\begin{aligned}
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}R^{-1}\sum_{k=1}^{N}y_ky_k^T-Nay^T-Nua^T+Naa^T\\
= R^{-1}\sum_{k=1}^{N}(y_k-a)(y_k-a)^T-R^{-1}N(a-u)(a-u)^T\\
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}(NR^{-1}Y + R^{-1}N(a-u)(a-u)^T)\\
= -\frac{N}{2}nlog(2\pi)-\frac{N}{2}log(det(R))- \frac{1}{2}(Ntr(R^{-1}Y) + N(a-u)R^{-1}(a-u)^T)
\end{aligned}
\end{equation*}

Set the gradient to zero to see $a$ and $R$ optimal values.
\begin{equation*}
\begin{aligned}
\nabla_a l(R,a) = -2R^{-1}(a-u) = 0\\
\therefore a=u \\
\nabla_R l(R,a) = -R^{-1} + R^{-1}(Y - (a-u)(a-u)^T)R^{-1} = 0\\
R = Y + (a-u)(a-u)^T\\
R = Y + (0)(0)^T\\
\therefore R=Y
\end{aligned}
\end{equation*}

\subsection*{7.8}

Express sign function as a probability where we order values with $y>1$ followed by $y<0$:
\begin{equation*}
\begin{aligned}
\prod_{i=1}^{k}prob(a_i^Tx + b_i + v_i > 0)\prod_{i=k+1}^{m}prob(a_i^Tx + b_i + v_i < 0)
\end{aligned}
\end{equation*}

Since $a_i$ and $b_i$ are known values, the only RV is the noise term. We can epxress $v_i$ as an expression of $a_i^Tx+b_i$. P represents the cumulative density function of $v_i$. We can represent the probability as follows:
\begin{equation*}
\begin{aligned}
\prod_{i=1}^{k}P(-a_i^Tx-b_i)\prod_{i=k+1}^{m}1-P(-a_i^Tx-b_i) 
\end{aligned}
\end{equation*}

Log likelihood below is concave so if we maximize, we obtain a convex problem:
\begin{equation*}
\begin{aligned}
l(x) = \sum_{i=1}^{k}log(P(-a_i^Tx-b_i))+\sum_{i=k+1}^{m}log(1-P(-a_i^Tx-b_i))\\ 
\end{aligned}
\end{equation*}

\subsection*{7.9}
Given $$y_i = f(a_i^Tx +b_i +v_i), i=1,\dots,m$$
We know that $a_i$ and $b_i$ are knowns, so lets expression the random variable $v_i$ as an expression of all other terms. We assume that $f$ is an invertible function.
$$v_i = f^{-1}(y_i)-a_i^Tx-b_i$$
The probability of observing $y_i,/dots,y_m$ is:
\begin{equation*}
\begin{aligned}
\prod_{i=1}^{m}prob(f^{-1}(y_i)-a_i^Tx-b_i)\\
l(x, f) = \sum_{i=1}^{m}log(prob(f^{-1}(y_i)-a_i^Tx-b_i))
\end{aligned}
\end{equation*}
This log probability is concave w.r.t x and f. Thus maximizing generates a convex optimization problem.\\\\

\textbf{Additional Exercises:}\\\\

\subsection*{3.9}
Given $$$$

The probability of observing $y_i,/dots,y_m$ is:
\begin{equation*}
\begin{aligned}
\prod_{i=1}^{m}prob(f^{-1}(y_i)-a_i^Tx-b_i)\\
l(x, f) = \sum_{i=1}^{m}log(prob(f^{-1}(y_i)-a_i^Tx-b_i))
\end{aligned}
\end{equation*}

% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}